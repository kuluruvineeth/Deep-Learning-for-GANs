{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing GAN Hacks to Train Stable Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deep Convolutional GANs(DCGANs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Downsample Using Strided Convolutions\n",
    "* The effect is the model will downsample the input from 64 x 64 to 32 x 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        1792      \n",
      "=================================================================\n",
      "Total params: 1,792\n",
      "Trainable params: 1,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# example of downsampling with strided convolutions\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64,(3,3),strides=(2,2),padding='same',input_shape=(64,64,3)))\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Upsample Using Strided Convolutions\n",
    "* The effect is the model will upsample the input from 64 x 64 to 128 x 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_transpose (Conv2DTran (None, 128, 128, 64)      3136      \n",
      "=================================================================\n",
      "Total params: 3,136\n",
      "Trainable params: 3,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# example of upsampling with strided convolutions\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2DTranspose\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv2DTranspose(64,(4,4),strides=(2,2),padding='same',input_shape=(64,64,3)))\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Using Leaky ReLU\n",
    "* Using the LeakyReLU with the default slope of 0.2 after a convolutional layer in a discriminator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "=================================================================\n",
      "Total params: 1,792\n",
      "Trainable params: 1,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# example of using leakyrelu in a discriminator model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import LeakyReLU\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64,(3,3),strides=(2,2),padding='same',input_shape=(64,64,3)))\n",
    "model.add(LeakyReLU(0.2))\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Using Batch Normalization\n",
    "* Batch Normalization standardizes the activations from a prior layer to have a zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "=================================================================\n",
      "Total params: 2,048\n",
      "Trainable params: 1,920\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# example of using batch norm in a discriminator model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64,(3,3),strides=(2,2),padding='same',input_shape=(64,64,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(0.2))\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Using Gaussian Weight Initialization\n",
    "* Initializing all weights using a zero-centered Gaussian distribution with the standard deviation of 0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_transpose_2 (Conv2DTr (None, 128, 128, 64)      3136      \n",
      "=================================================================\n",
      "Total params: 3,136\n",
      "Trainable params: 3,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# example of gaussian weight initialization in a generator model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.initializers import RandomNormal\n",
    "# define model\n",
    "model = Sequential()\n",
    "init = RandomNormal(mean=0.0,stddev=0.02)\n",
    "model.add(Conv2DTranspose(64,(4,4),strides=(2,2),padding='same',kernel_initializer=init,input_shape=(64,64,3)))\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Using Adam Stochastic Gradient Descent\n",
    "* Using the Adam version of stochastic gradient descent with the learning rate lr of 0.0002 and beta1 momentum value of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "=================================================================\n",
      "Total params: 1,792\n",
      "Trainable params: 1,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# example of using adam when training a discriminator model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64,(3,3),strides=(2,2),padding='same',input_shape=(64,64,3)))\n",
    "# compile model\n",
    "opt = Adam(lr=0.0002,beta_1=0.5)\n",
    "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Scale Images to the Range[-1,1]\n",
    "* Scaling a NumPy array of loaded image data to the required range of [-1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 255\n",
      "-1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# example of a function for scaling images\n",
    "from numpy.random import randint\n",
    "\n",
    "# scale image data from [0,255] to [-1,1]\n",
    "def scale_images(images):\n",
    "    # convert from unit8 to float32\n",
    "    images = images.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    images = (images - 127.5)/127.5\n",
    "    return images\n",
    "\n",
    "# define one 28x28 color image\n",
    "images = randint(0,256,28*28*3)\n",
    "images = images.reshape((1,28,28,3))\n",
    "# summarize pixel values\n",
    "print(images.min(),images.max())\n",
    "# scale\n",
    "scaled = scale_images(images)\n",
    "# summarize pixel scaled values\n",
    "print(scaled.min(),scaled.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Soumith Chintala's GAN Hacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Using a Gaussian Latent Space\n",
    "* The latent space defines the shape and distribution of the input to the generator model used to generate new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 100) -0.003627560248747541 0.9997837729083461\n"
     ]
    }
   ],
   "source": [
    "# example of sampling from a gaussian latent space\n",
    "from numpy.random import randn\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim,n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape((n_samples,latent_dim))\n",
    "    return x_input\n",
    "\n",
    "# size of latent space\n",
    "n_dim = 100\n",
    "# number of samples to generate\n",
    "n_samples = 500\n",
    "# generate samples\n",
    "samples = generate_latent_points(n_dim,n_samples)\n",
    "# summarize\n",
    "print(samples.shape,samples.mean(),samples.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1) 0.7000677288314175 1.1996506274030443\n"
     ]
    }
   ],
   "source": [
    "# example of positive label smoothing\n",
    "from numpy import ones\n",
    "from numpy.random import random\n",
    "\n",
    "# example of smoothing class=1 to [0.7,1.2]\n",
    "def smooth_positive_labels(y):\n",
    "    return y - 0.3 + (random(y.shape) * 0.5)\n",
    "\n",
    "# generate 'real' class labels (1)\n",
    "n_samples = 1000\n",
    "y = ones((n_samples,1))\n",
    "# smooth labels\n",
    "y = smooth_positive_labels(y)\n",
    "# summarize smooth labels\n",
    "print(y.shape,y.min(),y.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1) 0.0002911431007373011 0.29899450217592727\n"
     ]
    }
   ],
   "source": [
    "# example of negative label smoothing\n",
    "from numpy import zeros\n",
    "from numpy.random import random\n",
    "\n",
    "# example of smoothing class=0 to [0.0,0.3]\n",
    "def smooth_negative_labels(y):\n",
    "    return y + random(y.shape) * 0.3\n",
    "\n",
    "# generate 'fake' class labels (0)\n",
    "n_samples = 1000\n",
    "y = zeros((n_samples,1))\n",
    "# smooth labels\n",
    "y = smooth_negative_labels(y)\n",
    "# summarize smooth labels\n",
    "print(y.shape,y.min(),y.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Using Noisy Labels\n",
    "*  It is to introduce some errors to these labels where some fake images are marked as real, and some real images are marked as fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950.0\n",
      "49.0\n"
     ]
    }
   ],
   "source": [
    "# example of noisy labels\n",
    "from numpy import ones\n",
    "from numpy import zeros\n",
    "from numpy.random import choice\n",
    "\n",
    "# randomly flip some labels\n",
    "def noisy_labels(y,p_flip):\n",
    "    # determine the number of labels to flip\n",
    "    n_select = int(p_flip * y.shape[0])\n",
    "    # choose labels to flip\n",
    "    flip_ix = choice([i for i in range(y.shape[0])],size=n_select)\n",
    "    # invert the labels in place\n",
    "    y[flip_ix] = 1 - y[flip_ix]\n",
    "    return y\n",
    "\n",
    "# generate 'real' class labels(1)\n",
    "n_samples = 1000\n",
    "y = ones((n_samples,1))\n",
    "# flip labels with 5% probability\n",
    "y = noisy_labels(y,0.05)\n",
    "# summarize labels\n",
    "print(y.sum())\n",
    "\n",
    "# generate 'fake' class labels(0)\n",
    "y = zeros((n_samples,1))\n",
    "# flip labels with 5% probability\n",
    "y = noisy_labels(y,0.05)\n",
    "# summarize labels\n",
    "print(y.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
